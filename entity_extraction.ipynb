{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python-dotenv -> load environment variables from .env file\n",
    "!pip3 install --upgrade --quiet langchain langchain-community langchain-openai \n",
    "!pip3 install --upgrade --quiet python-dotenv\n",
    "!pip3 install --upgrade --quiet grobidarticleextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from GrobidArticleExtractor import GrobidArticleExtractor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from entity_extraction import ChatOpenRouter, parse_pdf, get_model_output, process_and_save_output, create_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.environ[\"OPENROUTER_API_KEY\"] \n",
    "GROBID_SERVER_URL = os.environ[\"GROBID_SERVER_URL\"]\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grobid_client = GrobidArticleExtractor(GROBID_SERVER_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phillips_text = parse_pdf('data/Phillips_2023_11pg.pdf', grobid_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_gpt_4o_mini = ChatOpenRouter('openai/gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_deepseek_v3 = ChatOpenRouter(model_name='deepseek/deepseek-chat-v3-0324:free')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_prompt = PromptTemplate.from_template(\"Task: Perform Named Entity Recognition (NER) with a deep understanding of neuroscience-specific terminology. Use precise and domain-relevant entity labels.\\\n",
    "Return the output in a training-ready JSON object for spaCy NER training, provide the entity,start position, end position, and respective label.\\\n",
    "Here is an example output: \\\n",
    "```json 'text': 'Drug addiction is a chronic','entities': ['entity': 'Drug addiction','start': 0,'end': 13,'label': 'DISORDER']```\\\n",
    "The text is provided here: {neuroscience_text}\")\n",
    "\n",
    "ner_chain = create_chain(llm_deepseek_v3, ner_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in content: 100%|██████████| 4/4 [04:04<00:00, 61.08s/it]\n",
      "Processing chunks in content: 100%|██████████| 1/1 [01:48<00:00, 108.20s/it]\n",
      "Processing chunks in content: 100%|██████████| 3/3 [04:06<00:00, 82.31s/it]\n",
      "Processing chunks in content: 100%|██████████| 1/1 [00:31<00:00, 31.73s/it]\n",
      "Processing chunks in content: 100%|██████████| 3/3 [01:57<00:00, 39.14s/it]\n",
      "Processing chunks in content: 100%|██████████| 2/2 [02:13<00:00, 66.76s/it]\n",
      "Processing chunks in content: 100%|██████████| 8/8 [03:51<00:00, 28.99s/it]\n",
      "Processing chunks in content: 100%|██████████| 1/1 [00:21<00:00, 21.31s/it]\n",
      "Processing chunks in content: 100%|██████████| 1/1 [00:20<00:00, 20.14s/it]\n",
      "Processing chunks in content: 100%|██████████| 1/1 [00:57<00:00, 57.80s/it]\n",
      "Processing chunks in content: 100%|██████████| 1/1 [00:17<00:00, 17.51s/it]\n",
      "Processing chunks in content: 100%|██████████| 1/1 [00:53<00:00, 53.88s/it]\n",
      "Running NER on sections: 100%|██████████| 12/12 [21:24<00:00, 107.06s/it]\n"
     ]
    }
   ],
   "source": [
    "all_output = []\n",
    "sections = phillips_text.get('sections', [])\n",
    "\n",
    "for section in tqdm(sections, desc=\"Running NER on sections\"):\n",
    "    for chunk in tqdm(section.get('content', []), desc=\"Processing chunks in content\"):\n",
    "        output = ner_chain.invoke({\"neuroscience_text\": chunk})\n",
    "        all_output.append(output)\n",
    "    for chunk in section.get('subsections', []):\n",
    "        output = ner_chain.invoke({\"neuroscience_text\": chunk})\n",
    "        all_output.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def process_and_save_output_multiple(output, file_prefix=\"output\", prompt=None):\n",
    "    \"\"\"\n",
    "    Process LLM output(s) to extract JSON, add model metadata and prompt once,\n",
    "    and save to a single JSON file.\n",
    "\n",
    "    Args:\n",
    "        output: A single LLM output or a list of LLM outputs (e.g., AIMessage) each with `content` and `response_metadata`.\n",
    "        file_prefix: Prefix for the output filename.\n",
    "        prompt: Optional PromptTemplate object with template and input_variables.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing metadata and all processed outputs.\n",
    "    \"\"\"\n",
    "    if not isinstance(output, list):\n",
    "        output = [output]\n",
    "\n",
    "    processed_outputs = []\n",
    "\n",
    "    for index, item in enumerate(output):\n",
    "        match = re.search(r\"```(?:json)?\\n(.*?)```\", item.content, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "            try:\n",
    "                json_obj = json.loads(json_str)\n",
    "                processed_outputs.append(json_obj)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Warning: Failed to decode JSON.\")\n",
    "        else:\n",
    "            print(f\"Warning: No JSON block found in section {index}.\")\n",
    "\n",
    "    if processed_outputs:\n",
    "        model_name = output[0].response_metadata.get(\"model_name\", \"unknown\")\n",
    "        if prompt:\n",
    "            prompt_metadata = {\n",
    "                \"template\": prompt.template,\n",
    "                \"input_variables\": prompt.input_variables\n",
    "            }\n",
    "        result = {\n",
    "            \"model_name\": model_name,\n",
    "            \"prompt\": prompt_metadata,\n",
    "            \"output\": processed_outputs\n",
    "        }\n",
    "\n",
    "\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        safe_model_name = model_name.replace('/', '_')\n",
    "        filename = f'{file_prefix}_{safe_model_name}_{timestamp}.json'\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "        print(f\"Saved {len(processed_outputs)} outputs to {filename}\")\n",
    "        return result\n",
    "    else:\n",
    "        print(\"No valid outputs to save.\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No JSON block found in section 11.\n",
      "Warning: No JSON block found in section 15.\n",
      "Warning: No JSON block found in section 16.\n",
      "Saved 24 outputs to zero-shot-ner_smaller-chunks_deepseek_deepseek-chat-v3-0324:free_20250502_185959.json\n"
     ]
    }
   ],
   "source": [
    "json_output = process_and_save_output_multiple(all_output, file_prefix = 'zero-shot-ner_smaller-chunks', prompt = ner_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
